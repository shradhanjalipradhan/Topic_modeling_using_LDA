{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b6fcb05b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\shrad\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1: don, people, just, think, like, good, know, time, did, right\n",
      "Topic #2: windows, thanks, file, dos, program, does, know, files, mail, use\n",
      "Topic #3: god, jesus, bible, believe, christ, christian, faith, christians, does, sin\n",
      "Topic #4: drive, scsi, card, ide, disk, hard, controller, drives, bus, floppy\n",
      "Topic #5: key, chip, encryption, clipper, government, keys, escrow, use, law, algorithm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "\n",
    "# Fetch the 20 newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Use TF-IDF vectorizer to convert text data into numerical vectors\n",
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "X = vectorizer.fit_transform(newsgroups.data)\n",
    "\n",
    "# Apply Non-Negative Matrix Factorization (NMF) to identify topics\n",
    "n_topics = 5  # You can adjust the number of topics based on your preference\n",
    "nmf = NMF(n_components=n_topics, random_state=1)\n",
    "nmf.fit(X)\n",
    "\n",
    "# Display the top words for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    top_words_idx = topic.argsort()[:-10 - 1:-1]  # Display top 10 words for each topic\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"Topic #{topic_idx + 1}: {', '.join(top_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df637ec1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shrad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\shrad\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1: peopl, god, say, think, just, christian, believ, did, know, like\n",
      "Topic #2: window, file, thank, program, use, run, know, doe, do, help\n",
      "Topic #3: game, team, play, year, player, win, hockey, basebal, fan, score\n",
      "Topic #4: drive, card, disk, scsi, hard, control, ide, use, monitor, floppi\n",
      "Topic #5: key, chip, encrypt, use, clipper, govern, phone, secur, escrow, algorithm\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Fetch the 20 newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Tokenization, removing punctuation, lowercase conversion, removing stop words, and stemming\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Removing punctuation and converting to lowercase\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Removing stop words\n",
    "    tokens = [word for word in tokens if word not in ENGLISH_STOP_WORDS]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to all documents\n",
    "preprocessed_data = [preprocess_text(doc) for doc in newsgroups.data]\n",
    "\n",
    "# Use TF-IDF vectorizer to convert preprocessed text data into numerical vectors\n",
    "vectorizer = TfidfVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_data)\n",
    "\n",
    "# Apply Non-Negative Matrix Factorization (NMF) to identify topics\n",
    "n_topics = 5  # You can adjust the number of topics based on your preference\n",
    "nmf = NMF(n_components=n_topics, random_state=1)\n",
    "nmf.fit(X)\n",
    "\n",
    "# Display the top words for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    top_words_idx = topic.argsort()[:-10 - 1:-1]  # Display top 10 words for each topic\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"Topic #{topic_idx + 1}: {', '.join(top_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "105fe849",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\shrad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "C:\\Users\\shrad\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic #1: max, bhj, giz, gk, bj, wm, qax, kn, ax, nrhj\n",
      "Topic #2: use, file, program, includ, avail, window, inform, server, data, run\n",
      "Topic #3: peopl, say, did, know, said, think, god, just, time, presid\n",
      "Topic #4: db, mov, cs, bh, byte, bit, al, si, di, bl\n",
      "Topic #5: imag, jpeg, file, gif, format, color, version, use, program, display\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Fetch the 20 newsgroups dataset\n",
    "newsgroups = fetch_20newsgroups(subset='all', remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Tokenization, removing punctuation, lowercase conversion, removing stop words, and stemming\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    tokens = nltk.word_tokenize(text)\n",
    "    \n",
    "    # Removing punctuation and converting to lowercase\n",
    "    tokens = [word.lower() for word in tokens if word.isalpha()]\n",
    "    \n",
    "    # Removing stop words\n",
    "    tokens = [word for word in tokens if word not in ENGLISH_STOP_WORDS]\n",
    "    \n",
    "    # Stemming\n",
    "    stemmer = PorterStemmer()\n",
    "    tokens = [stemmer.stem(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(tokens)\n",
    "\n",
    "# Apply preprocessing to all documents\n",
    "preprocessed_data = [preprocess_text(doc) for doc in newsgroups.data]\n",
    "\n",
    "# Use CountVectorizer to create a bag-of-words representation\n",
    "vectorizer = CountVectorizer()\n",
    "X = vectorizer.fit_transform(preprocessed_data)\n",
    "\n",
    "# Apply Non-Negative Matrix Factorization (NMF) to identify topics\n",
    "n_topics = 5  # You can adjust the number of topics based on your preference\n",
    "nmf = NMF(n_components=n_topics, random_state=1)\n",
    "nmf.fit(X)\n",
    "\n",
    "# Display the top words for each topic\n",
    "feature_names = vectorizer.get_feature_names_out()\n",
    "for topic_idx, topic in enumerate(nmf.components_):\n",
    "    top_words_idx = topic.argsort()[:-10 - 1:-1]  # Display top 10 words for each topic\n",
    "    top_words = [feature_names[i] for i in top_words_idx]\n",
    "    print(f\"Topic #{topic_idx + 1}: {', '.join(top_words)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "28be1af8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shrad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d6f155e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\shrad\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "wonder anyon could enlighten car saw sport look late earli call door realli front bumper separ rest anyon tellm model engin year car whatev info funki look pleas\n",
      "fair number brave soul upgrad si clock oscil share experi pleas send brief messag detail experi top speed cpu rate add card heat hour usag per floppi disk function floppi especi summar next two pleas add network knowledg base done clock upgrad answer\n",
      "well mac plu final gave ghost weekend start life way back market new machin bit sooner intend look pick powerbook mayb bunch question somebodi anybodi know dirt next round powerbook introduct heard suppos make appear heard anymor sinc access wonder anybodi anybodi heard rumor price drop powerbook line like one went impress display could probabl swing got disk rather realli feel much display look great realli could solicit opinion peopl use worth take disk size money hit get activ realiz real subject play around machin comput store breifli figur opinion somebodi actual use machin daili might prove well hellcat thank bunch advanc info could post summari read time premium final around tom willi purdu electr engin\n",
      "like get inform\n",
      "articl understand basic known bug warn system softwar thing check right valu yet set till rather fix code possibl introduc new tell crew see warn ignor\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "import string\n",
    "import nltk\n",
    "\n",
    "# Download the 'stopwords' resource\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load dataset\n",
    "data = fetch_20newsgroups(remove=('headers', 'footers', 'quotes'))\n",
    "\n",
    "# Extract the features\n",
    "count_vectorizer = CountVectorizer(stop_words='english')\n",
    "features = count_vectorizer.fit_transform(data.data)\n",
    "\n",
    "# Define additional preprocessing steps\n",
    "def preprocess_text(text):\n",
    "    # Tokenization\n",
    "    words = text.split()\n",
    "\n",
    "    # Remove all punctuation and lowercase words\n",
    "    words = [word.lower() for word in words if word.isalpha()]\n",
    "\n",
    "    # Remove the stop words\n",
    "    words = [word for word in words if word not in stopwords.words('english')]\n",
    "\n",
    "    # Stemming\n",
    "    ps = PorterStemmer()\n",
    "    words = [ps.stem(word) for word in words]\n",
    "\n",
    "    return words\n",
    "\n",
    "# Apply preprocessing to each document\n",
    "data.data = [preprocess_text(text) for text in data.data]\n",
    "\n",
    "# Print first 5 documents after preprocessing\n",
    "for i in range(5):\n",
    "    print(\" \".join(data.data[i]))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
